---
title: "Identifying the Performance of Dumbbell Lifts from Personal Fitness Sensors"
author: "Jimi Damon"
date: "12/04/2016"
output: html_document
hitheme: tomorrow
mode: selfcontained
highlighter: highlight.js
framework: io2012
url:
  assets: ../../assets
  lib: ../../librariesNew
widgets: mathjax
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='./figure/',
                     warning=FALSE, message=FALSE)
load("/home/jdamon/Projects/coursera_courses/machine_learning/.RData")
```
# Background of the data

One thing that people regularly do is quantify how much of a
particular activity they do, but they rarely quantify how well they do
it. In this project, your goal will be to use data from accelerometers
on the belt, forearm, arm, and dumbell of 6 participants. They were
asked to perform barbell lifts correctly and incorrectly in 5
different ways.  

The classifications for the various ways of lifing dumbbell was as
follows:  Class 'A' corresponded to exactly following the specification for
lifting dumbbell, class 'B' indicated throwing the elbows to the
front, class 'C' indicated only lifting the dumbbell halfway, class
'D' indicated lowering the dumbbell only halfway and class 'E'
indicated throwing the hips to the front while performing a lift.

# Reading the data

I downloaded the training and testing datasets from the URL
https://d396qusza40orc.cloudfront.net/predmachlearn, and saved them to
my local directory. The files / data sets were called
"pml-training.csv" and "pml-testing.csv".

```{r,echo=TRUE}
library(ggplot2)
library(caret)
library(randomForest); 
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

Exploring the data I found the dimensions of the training data to be 
```{r,echo=FALSE}
dim(training)
```


# Data cleaning

I began examining the data and discovered that there were numerous 
columns that contained NA values ( i.e. kurtosis_roll_belt ) . In addition, there were a number of
variables as well that had the same value repeated for the length of
the data frame ( i.e. new_window ). So, I decided that my first method for cleaning the
data was to first remove the zero variance elements and then to remove
the columns that corresponded to the timestamps and indices that had
nothing to do with the training or method of lifting.

```{r,echo=TRUE}
nzv_cols <- nearZeroVar( training)
modtraining <- training[,-nzv_cols]
oknames <- names(modtraining)[colSums(is.na(modtraining)) == 0]
nnames <- oknames[grep(".*(num_window|X|timestamp|user_name).*",oknames,invert=TRUE)]
nnames
```

# Formula creation

Now I was left with a new variable nnames that contained the filtered
list of names.  I then created a formula for these fields only.
```{r,echo=TRUE}
form <- as.formula(paste("classe ~ ",paste(nnames[-length(nnames)],collapse=" + ")))
form
```

# Data segmentation

```{r,echo=TRUE}
set.seed(69)
dataTrain <- createDataPartition(trainData$classe, p = 0.7, list = FALSE)
train <- trainData[dataTrain,]
valid <- trainData[-dataTrain,]
```

# Training and cross-validation

I went with a random forest approach to my data model. The reason for this was due to the perceived non-linear aspect of the data in question. I decided to use 5-fold cross-validation for my model due to the fact that it corresponds to a shorter simulation cycle and during my analysis I was runing into severe problems with simulation times balooning.  The training control was set up as follows:

```{r,echo=TRUE}
control <- trainControl(method = "cv", number = 5)
```

In addition, nodata transformations were applied to the data as they are considered less
important in non-linear model fitting like random forests. I ran the training method on the "train" subset of the original training data.


```{r,echo=TRUE,eval=FALSE}
set.seed(42)
fit_rf <- train( form, data=train, method="rf", trControl=control ,
                do.trace=TRUE )
```

```{r,echo=FALSE}
fit_rf <- mymodel
print(fit_rf)
```

# Out of sample calculation

```{r,echo=TRUE}
predata <- predict(mymodel,valid)
confmatrix <- confusionMatrix(valid$classe, predata )
(accuracy <- confmatrix$overall[1])
```
So, for accuracy of 0.991, the out of sample error rate is around .009, or less than 1%.

# Predicting 20 different test cases.

Now I applied this model to the testing data that was provided by the website

```{r,echo=TRUE}
(predict(mymodel, testing))
```

